{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1rACtuiinMRPFKca__wUMjYINGcMCbAOj","authorship_tag":"ABX9TyNos1o8JGVesNaDXS3T0rNQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"L1anKIFS48Zp"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import Sequence\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n","from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n","from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","import warnings\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from textwrap import wrap\n","\n","plt.rcParams['font.size'] = 12\n","sns.set_style(\"dark\")\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/tensorfrem"],"metadata":{"id":"JFhm-mKbgw7V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = \"/content/drive/MyDrive/tensorfrem/Flicker8k_Dataset\""],"metadata":{"id":"QrS068JI95lH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/tensorfrem/captions.txt\")\n","\n","data.head()"],"metadata":{"id":"QPGgq_g_99-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def readImage(path,img_size=224):\n","    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n","    img = img_to_array(img)\n","    img = img/255.\n","\n","    return img\n","\n","def display_images(temp_df):\n","    temp_df = temp_df.reset_index(drop=True)\n","    plt.figure(figsize = (20 , 20))\n","    n = 0\n","    for i in range(15):\n","        n+=1\n","        plt.subplot(5 , 5, n)\n","        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n","        image = readImage(f\"Flicker8k_Dataset/{temp_df.image[i]}\")\n","        plt.imshow(image)\n","        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n","        plt.axis(\"off\")"],"metadata":{"id":"ZBmMBjz6-G5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_images(data.sample(15))"],"metadata":{"id":"og0TBo1z-gDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_preprocessing(data):\n","    data['caption'] = data['caption'].apply(lambda x: x.lower())\n","    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n","    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n","    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n","    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n","    return data"],"metadata":{"id":"cqP1on8c-_OV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = text_preprocessing(data)\n","captions = data['caption'].tolist()\n","captions[:10]"],"metadata":{"id":"htEYa5zF_GbD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(captions)\n","vocab_size = len(tokenizer.word_index) + 1\n","max_length = max(len(caption.split()) for caption in captions)\n","\n","images = data['image'].unique().tolist()\n","nimages = len(images)\n","\n","split_index = round(0.85*nimages)\n","train_images = images[:split_index]\n","val_images = images[split_index:]\n","\n","train = data[data['image'].isin(train_images)]\n","test = data[data['image'].isin(val_images)]\n","\n","train.reset_index(inplace=True,drop=True)\n","test.reset_index(inplace=True,drop=True)\n","\n","tokenizer.texts_to_sequences([captions[1]])[0]"],"metadata":{"id":"AdoIluNW_P9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DenseNet201()\n","fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n","\n","img_size = 224\n","features = {}\n","for image in tqdm(data['image'].unique().tolist()):\n","    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n","    img = img_to_array(img)\n","    img = img/255.\n","    img = np.expand_dims(img,axis=0)\n","    feature = fe.predict(img, verbose=0)\n","    features[image] = feature"],"metadata":{"id":"-RmMLYb1_ekJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","pickle.dump(features, open(os.path.join( 'features.pkl'), 'wb'))"],"metadata":{"id":"Ll0ysJLecwkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load features from pickle\n","with open('features.pkl', 'rb') as f:\n","    features = pickle.load(f)"],"metadata":{"id":"5kxyJM_AcwLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataGenerator(Sequence):\n","\n","    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n","                 vocab_size, max_length, features,shuffle=True):\n","\n","        self.df = df.copy()\n","        self.X_col = X_col\n","        self.y_col = y_col\n","        self.directory = directory\n","        self.batch_size = batch_size\n","        self.tokenizer = tokenizer\n","        self.vocab_size = vocab_size\n","        self.max_length = max_length\n","        self.features = features\n","        self.shuffle = shuffle\n","        self.n = len(self.df)\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            self.df = self.df.sample(frac=1).reset_index(drop=True)\n","\n","    def __len__(self):\n","        return self.n // self.batch_size\n","\n","    def __getitem__(self,index):\n","\n","        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n","        X1, X2, y = self.__get_data(batch)\n","        return (X1, X2), y\n","\n","    def __get_data(self,batch):\n","\n","        X1, X2, y = list(), list(), list()\n","\n","        images = batch[self.X_col].tolist()\n","\n","        for image in images:\n","            feature = self.features[image][0]\n","\n","            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n","            for caption in captions:\n","                seq = self.tokenizer.texts_to_sequences([caption])[0]\n","\n","                for i in range(1,len(seq)):\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n","                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n","                    X1.append(feature)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","\n","        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","\n","        return X1, X2, y"],"metadata":{"id":"kcwvDynTaIRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input1 = Input(shape=(1920,))\n","input2 = Input(shape=(max_length,))\n","\n","img_features = Dense(256, activation='relu')(input1)\n","img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n","\n","sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n","merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n","sentence_features = LSTM(256)(merged)\n","x = Dropout(0.5)(sentence_features)\n","x = add([x, img_features])\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","output = Dense(vocab_size, activation='softmax')(x)\n","\n","caption_model = Model(inputs=[input1,input2], outputs=output)\n","caption_model.compile(loss='categorical_crossentropy',optimizer='adam',\n","                      metrics=['accuracy'])"],"metadata":{"id":"ZmzXXOKTaLMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import plot_model"],"metadata":{"id":"-1JM-ppmaLJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_model(caption_model)"],"metadata":{"id":"h5ax3AwRaLGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["caption_model.summary()"],"metadata":{"id":"P9YwfG4_aLAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n","                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n","\n","validation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n","                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)"],"metadata":{"id":"mhbSE9QgaK87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"BAS_modeles.h5\"\n","checkpoint = ModelCheckpoint(model_name,\n","                            monitor=\"val_loss\",\n","                            mode=\"min\",\n","                            save_best_only = True,\n","                            verbose=1)\n","\n","earlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n","\n","learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n","                                            patience=3,\n","                                            verbose=1,\n","                                            factor=0.2,\n","                                            min_lr=0.00000001)"],"metadata":{"id":"5vg3Jj-AaK5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = caption_model.fit(\n","        train_generator,\n","        epochs=30,\n","        validation_data=validation_generator,\n","        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"],"metadata":{"id":"0OY4S5u2aK3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss, train_accuracy = caption_model.evaluate(train_generator, verbose=1)\n","print(f'Training Loss: {train_loss:.4f}')\n","print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n","\n","# Evaluate accuracy on the validation data\n","val_loss, val_accuracy = caption_model.evaluate(validation_generator, verbose=1)\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')"],"metadata":{"id":"_R6KE6MT5EnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["caption_model.save('caption_modelesss.h5')"],"metadata":{"id":"Jsu1qNXFbciJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20,8))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"QmrDuq6GbxQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20, 8))\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"qbhIW3wMnNZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('modeles.h5')\n"],"metadata":{"id":"RCyF9qpFM70d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def idx_to_word(integer,tokenizer):\n","\n","    for word, index in tokenizer.word_index.items():\n","        if index==integer:\n","            return word\n","    return None"],"metadata":{"id":"D_6tibA2_8Hf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_caption(model, image, tokenizer, max_length, features):\n","\n","    feature = features[image]\n","    in_text = \"startseq\"\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], max_length)\n","\n","        y_pred = model.predict([feature,sequence])\n","        y_pred = np.argmax(y_pred)\n","\n","        word = idx_to_word(y_pred, tokenizer)\n","\n","        if word is None:\n","            break\n","\n","        in_text+= \" \" + word\n","\n","        if word == 'endseq':\n","            break\n","\n","    return in_text"],"metadata":{"id":"Rz_DFnMIbxgO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples = test.sample(15)\n","samples.reset_index(drop=True,inplace=True)"],"metadata":{"id":"eZNXX_SHbxmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index,record in samples.iterrows():\n","\n","    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n","    img = img_to_array(img)\n","    img = img/255.\n","\n","    caption = predict_caption(model, record['image'], tokenizer, max_length, features)\n","    samples.loc[index,'caption'] = caption"],"metadata":{"id":"zMtdsnqabxta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_images(samples)"],"metadata":{"id":"YuXuIIrBcPCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('caption_model.h5')"],"metadata":{"id":"UNAsCUG4a8T6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index,record in samples.iterrows():\n","\n","    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n","    img = img_to_array(img)\n","    img = img/255.\n","\n","    caption = predict_caption(model, record['image'], tokenizer, max_length, features)\n","    samples.loc[index,'caption'] = caption"],"metadata":{"id":"TxYRPHoYo1Up"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_images(samples)"],"metadata":{"id":"PjgqmK1VqSRa"},"execution_count":null,"outputs":[]}]}