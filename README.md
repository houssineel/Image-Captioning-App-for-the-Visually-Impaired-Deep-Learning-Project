# ğŸ“± Image Captioning App for the Visually Impaired

## ğŸ¯ Objectif du projet
DÃ©velopper une application mobile qui gÃ©nÃ¨re automatiquement des lÃ©gendes pour les images afin dâ€™amÃ©liorer lâ€™accessibilitÃ© des personnes non-voyantes.  
Lâ€™application capture une image via la camÃ©ra du smartphone et produit une **description audio** en langage naturel.

---

## ğŸ§  MÃ©thodologie
- **CNN (VGG16, DenseNet201)** : extraction des caractÃ©ristiques visuelles.  
- **RNN & LSTM** : gÃ©nÃ©ration des lÃ©gendes textuelles.  
- **Transfer Learning** : rÃ©utilisation de modÃ¨les prÃ©-entraÃ®nÃ©s pour amÃ©liorer la prÃ©cision.  
- **Dataset** : Flickr8k (images + lÃ©gendes).  
- **Ã‰valuation** : BLEU Score pour mesurer la qualitÃ© des descriptions.  

---

## ğŸ› ï¸ Outils & Technologies
- **Langage** : Python  
- **Frameworks** : PyTorch, TensorFlow, Keras  
- **Librairies** : NumPy, Pandas, Matplotlib, Seaborn, Pickle, Tqdm  
- **Environnements** : Jupyter Notebook, Google Colab  
- **DÃ©ploiement mobile** : Android Studio + TensorFlow Lite  

---

## ğŸ“Š RÃ©sultats
- Comparaison entre VGG16 et DenseNet201.  
- VGG16 : bonne prÃ©cision mais plus lourd Ã  entraÃ®ner.  
- DenseNet201 : meilleur compromis entre prÃ©cision et rapiditÃ©.  
- BLEU Score utilisÃ© pour valider les performances.  

---

## ğŸš€ Perspectives
- AmÃ©liorer les performances avec des modÃ¨les plus rÃ©cents (Transformers/ViT).  
- Ã‰tendre le dataset pour une meilleure gÃ©nÃ©ralisation.  
- IntÃ©grer un module de synthÃ¨se vocale multilingue pour rendre lâ€™application plus inclusive.  

---

## ğŸ¥ DÃ©monstration du projet
[â–¶ï¸ Cliquez ici pour voir la vidÃ©o](video.mp4)




