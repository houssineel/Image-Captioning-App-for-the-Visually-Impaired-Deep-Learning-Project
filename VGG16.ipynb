{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"mount_file_id":"1ukhkqlpsGT4mo8xwUO49bCSKp0O1I2gA","authorship_tag":"ABX9TyNxTGqd8X3pDZjYHhFUkk/2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nwJW5gCHXVrX"},"outputs":[],"source":["cd /content/drive/MyDrive/keras"]},{"cell_type":"code","source":["import os\n","import pickle\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"],"metadata":{"id":"uPgoyXxjX-ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BASE_DIR = 'Flicker8k_Dataset'#Flicker8k_Dataset\n","WORKING_DIR = 'Flickr8k_text'#Flickr8k_text"],"metadata":{"id":"rLQWdbTIYEwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load vgg16 model\n","model = VGG16()\n","# restructure the model\n","model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","# summarize\n","print(model.summary())"],"metadata":{"id":"_7NrT6IKYEs8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract features from image\n","features = {}\n","directory = 'Flickr8k_Dataset/Flicker8k_Dataset'\n","\n","for img_name in tqdm(os.listdir(directory)):\n","    # load the image from file\n","    img_path = directory + '/' + img_name\n","    image = load_img(img_path, target_size=(224, 224))\n","    # convert image pixels to numpy array\n","    image = img_to_array(image)\n","    # reshape data for model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # preprocess image for vgg\n","    image = preprocess_input(image)\n","    # extract features\n","    feature = model.predict(image, verbose=0)\n","    # get image ID\n","    image_id = img_name.split('.')[0]\n","    # store feature\n","    features[image_id] = feature"],"metadata":{"id":"708Y53a0YEqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# store features in pickle\n","pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"],"metadata":{"id":"zXNba7BIYEnb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load features from pickle\n","with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n","    features = pickle.load(f)"],"metadata":{"id":"DF-hbQmvYEkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open( 'captions.txt', 'r') as f:\n","    next(f)\n","    captions_doc = f.read()"],"metadata":{"id":"XKM5yycjYEhs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create mapping of image to captions\n","mapping = {}\n","# process lines\n","for line in tqdm(captions_doc.split('\\n')):\n","    # split the line by comma(,)\n","    tokens = line.split(',')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    # remove extension from image ID\n","    image_id = image_id.split('.')[0]\n","    # convert caption list to string\n","    caption = \" \".join(caption)\n","    # create list if needed\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    # store the caption\n","    mapping[image_id].append(caption)"],"metadata":{"id":"MAhDWF6JYEe4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(mapping)"],"metadata":{"id":"xixMDHwgYEb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean(mapping):\n","    for key, captions in mapping.items():\n","        for i in range(len(captions)):\n","            # take one caption at a time\n","            caption = captions[i]\n","            # preprocessing steps\n","            # convert to lowercase\n","            caption = caption.lower()\n","            # delete digits, special chars, etc.,\n","            caption = caption.replace('[^A-Za-z]', '')\n","            # delete additional spaces\n","            caption = caption.replace('\\s+', ' ')\n","            # add start and end tags to the caption\n","            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n","            captions[i] = caption"],"metadata":{"id":"WdnzzAl7YEYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# before preprocess of text\n","mapping['1000268201_693b08cb0e']"],"metadata":{"id":"SftW7KHTwzYc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess the text\n","clean(mapping)"],"metadata":{"id":"5Qw33XXewzU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_captions = []\n","for key in mapping:\n","    for caption in mapping[key]:\n","        all_captions.append(caption)"],"metadata":{"id":"ffV1EZvbwzR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(all_captions)"],"metadata":{"id":"1NaqvU0PwzOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_captions[:10]"],"metadata":{"id":"Zgs5qCQUwzLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_captions)\n","vocab_size = len(tokenizer.word_index) + 1"],"metadata":{"id":"ZrnPov40wzIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size"],"metadata":{"id":"eL2uyqjPwzFj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get maximum length of the caption available\n","max_length = max(len(caption.split()) for caption in all_captions)\n","max_length"],"metadata":{"id":"8o6K8CwywzCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_ids = list(mapping.keys())\n","split = int(len(image_ids) * 0.90)\n","train = image_ids[:split]\n","test = image_ids[split:]"],"metadata":{"id":"AfW8_iEgwy_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create data generator to get data in batch (avoids session crash)\n","def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n","    # loop over images\n","    X1, X2, y = list(), list(), list()\n","    n = 0\n","    while 1:\n","        for key in data_keys:\n","            n += 1\n","            captions = mapping[key]\n","            # process each caption\n","            for caption in captions:\n","                # encode the sequence\n","                seq = tokenizer.texts_to_sequences([caption])[0]\n","                # split the sequence into X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pairs\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\n","                    # store the sequences\n","                    X1.append(features[key][0])\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            if n == batch_size:\n","                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","                yield {\"image\": X1, \"text\": X2}, y\n","                X1, X2, y = list(), list(), list()\n","                n = 0"],"metadata":{"id":"k-eT_6itwy8q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, add\n","from tensorflow.keras.models import Model\n","\n","# encoder model\n","# image feature layers\n","inputs1 = Input(shape=(4096,), name=\"image\")\n","fe1 = Dropout(0.4)(inputs1)  # Ajout de Dropout pour la régularisation\n","fe2 = Dense(256, activation='relu')(fe1)\n","fe3 = Dense(128, activation='relu')(fe2)  # Couches supplémentaires pour une complexité accrue\n","\n","# sequence feature layers\n","inputs2 = Input(shape=(max_length,), name=\"text\")\n","se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","se2 = Dropout(0.4)(se1)  # Ajout de Dropout pour la régularisation\n","se3 = LSTM(256)(se2)\n","se4 = Dense(128, activation='relu')(se3)  # Couches supplémentaires pour une complexité accrue\n","\n","# decoder model\n","decoder1 = add([fe3, se4])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","decoder3 = Dense(128, activation='relu')(decoder2)  # Couches supplémentaires pour une complexité accrue\n","outputs = Dense(vocab_size, activation='softmax')(decoder3)\n","\n","# création du modèle\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Afficher le résumé du modèle\n","model.summary()\n"],"metadata":{"id":"SXKlTC12wy5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","batch_size = 32\n","steps = len(train) // batch_size\n","val_steps = len(test) // batch_size\n","\n","# Définir les générateurs de données\n","val_generator = data_generator(test, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n","generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n","\n","# Entraîner le modèle\n","history = model.fit(generator, epochs=10, steps_per_epoch=steps, verbose=1,\n","                    validation_data=val_generator, validation_steps=val_steps)\n","\n","# Sauvegarder le modèle\n","model.save('path_to_save_model/model.h5')\n","\n","\n"],"metadata":{"id":"pN9xXJl8xbq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras.models import load_model\n","model = load_model('models/model19.h5')"],"metadata":{"id":"jVgmsuYuUC1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(20,8))\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"dkSjb2LiezjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20, 8))\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Val'], loc='upper left')\n","plt.show()"],"metadata":{"id":"Bv9n6RUneqVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from keras.models import load_model\n","model = load_model('models/model19.h5')\n","\n"],"metadata":{"id":"xzTBKMTosT_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def idx_to_word(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None"],"metadata":{"id":"zGyuwUw4sT4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate caption for an image\n","def predict_caption(model, image, tokenizer, max_length):\n","    # add start tag for generation process\n","    in_text = 'startseq'\n","    # iterate over the max length of sequence\n","    for i in range(max_length):\n","        # encode input sequence\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        # pad the sequence\n","        sequence = pad_sequences([sequence], max_length)\n","        # predict next word\n","        yhat = model.predict([image, sequence], verbose=0)\n","        # get index with high probability\n","        yhat = np.argmax(yhat)\n","        # convert index to word\n","        word = idx_to_word(yhat, tokenizer)\n","        # stop if word not found\n","        if word is None:\n","            break\n","        # append word as input for generating next word\n","        in_text += \" \" + word\n","        # stop if we reach end tag\n","        if word == 'endseq':\n","            break\n","\n","    return in_text"],"metadata":{"id":"UOoFNgnbxbnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","from keras.models import load_model\n","modele = load_model('models/model19.h5')\n","def generate_caption(image_name):\n","    # load the image\n","    # image_name = \"1001773457_577c3a7d70.jpg\"\n","    image_id = image_name.split('.')[0]\n","    img_path =image_name\n","    image = Image.open(img_path)\n","    captions = mapping[image_id]\n","    print('---------------------Actual---------------------')\n","    for caption in captions:\n","        print(caption)\n","    # predict the caption\n","    y_pred = predict_caption(modele, features[image_id], tokenizer, max_length)\n","    print('--------------------Predicted--------------------')\n","    print(y_pred)\n","    plt.imshow(image)"],"metadata":{"id":"i_Ih5_TZxbiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu\n","# validate with test data\n","actual, predicted = list(), list()\n","\n","for key in tqdm(test):\n","    # get actual caption\n","    captions = mapping[key]\n","    # predict the caption for image\n","    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n","    # split into words\n","    actual_captions = [caption.split() for caption in captions]\n","    y_pred = y_pred.split()\n","    # append to the list\n","    actual.append(actual_captions)\n","    predicted.append(y_pred)\n","\n"],"metadata":{"id":"RgbneYQgFq7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calcuate BLEU score\n","print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","print(\"BLEU-3: %f\" % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0)))\n","\n"],"metadata":{"id":"LFD-4HF2KW6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_caption('10815824_2997e03d76.jpg')\n","#/content/drive/MyDrive/keras/Flickr8k_Dataset/Flicker8k_Dataset\n"],"metadata":{"id":"2EUneOLXxbWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('model1.h5')"],"metadata":{"id":"aE2qHxlEXItM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_model = VGG16()\n","# restructure the model\n","vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)"],"metadata":{"id":"_VdqXQMqpUSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#generate_caption('eliott-reyna-jCEpN62oWL4-unsplash.jpg')\n","#from keras.models import load_model\n","#model = load_model('models/model19.h5')\n","#/content/drive/MyDrive/keras/12.jpg\n","image_path = '/content/5 (1).jpg'\n","# load image\n","image = load_img(image_path, target_size=(224, 224))\n","# convert image pixels to numpy array\n","image = img_to_array(image)\n","# reshape data for model\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","# preprocess image for vgg\n","image = preprocess_input(image)\n","# extract features\n","feature = vgg_model.predict(image, verbose=0)\n","# predict from the trained model\n","predict_caption(model, feature, tokenizer, max_length)"],"metadata":{"id":"bzgRURICyGYr"},"execution_count":null,"outputs":[]}]}